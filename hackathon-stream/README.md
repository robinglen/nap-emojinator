# Hackathon stream

## 26/01/2017 - 8:30 Starting up

I want to play around with Google vision, so rough idea is I'm going to use the edit archive api to collect all the cover images. Then use some facial recognition to break down the human condition into emoji. I will collect that data and compare it to conversation/click through or something to pretend it has business value.
